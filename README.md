# ğŸ“ˆ Stock Market ETL Pipeline using Apache Airflow (Dockerized)

An end-to-end data engineering ETL pipeline built with Apache Airflow, MinIO (S3-compatible object storage), and PostgreSQL, fully containerized using Docker.
The pipeline fetches daily stock prices from an external API, processes and stores intermediate data in object storage, and loads curated data into a data warehouse for analytics.

## ğŸ” Project Motivation

Financial market data is:

Time-sensitive

API-driven

Often requires multi-stage transformation

Needs reliable orchestration and monitoring

This project demonstrates how a real-world production ETL pipeline can be built using industry-standard tools, focusing on:

Workflow orchestration

Cloud-native storage patterns

Data reliability and observability

Clean separation of raw, processed, and curated data

## ğŸ—ï¸ High-Level Architecture

Technology Stack

Apache Airflow â€“ Workflow orchestration

Docker & Docker Compose â€“ Containerization

MinIO â€“ S3-compatible object storage

PostgreSQL â€“ Data warehouse

Python â€“ API ingestion 

Spark - transformation logic

Metaflow - Dashboarding



## Airflow DAG

![Airflow DAG Graph](airflow_dag_graph.png)
![Airflow Connections](Airflow_connections.png)


## Infrastructure Overview

![Docker Containers](Docker_containers.png)
![MinIO Storage](Minio_data_storage.png)
![Postgres Data](Postgres_data.png)

ğŸ”„ Data Pipeline Flow
DAG: stock_market

The pipeline executes the following steps:

Step	Task	Description
1	is_api_available	Sensor checks if the stock API endpoint is reachable
2	get_stock_prices	Fetches stock price data (example: NVDA)
3	store_prices	Stores raw API response in MinIO
4	format_prices	Cleans and structures the raw data
5	get_formatted_csv	Converts processed data into CSV format
6	load_to_dw	Loads final data into PostgreSQL

## ğŸ§  Key Design Decisions

### âœ… Sensor-based API validation

    Prevents unnecessary failures

    Ensures downstream tasks only run when data is available

### âœ… Object storage (MinIO) for intermediate data

    Decouples ingestion from transformation

    Enables reprocessing without re-calling the API

    Mimics real cloud S3 architectures

### âœ… Dockerized environment

    Ensures reproducibility

    Easy local setup

    Matches production-like workflows


## ğŸ§© Airflow Connections Configuration

Connections Used
1ï¸âƒ£ Stock API (stock_api)

Type: HTTP

Stores API host, endpoint, and headers

Accessed dynamically using BaseHook

2ï¸âƒ£ MinIO (S3 Compatible)

Uses AWS connection type

Custom endpoint pointing to MinIO

Enables use of S3ToSqlOperator

3ï¸âƒ£ PostgreSQL (Data Warehouse)

Stores curated stock price data

Used as final analytical storage

ğŸª£ Object Storage â€“ MinIO

Bucket Structure

stock-market/

â”œâ”€â”€ raw/
â”‚   â””â”€â”€ stock_prices_YYYY-MM-DD.json

â”œâ”€â”€ processed/
â”‚   â””â”€â”€ stock_prices_YYYY-MM-DD.csv


This structure mirrors medallion architecture principles:

Raw â†’ immutable API data

Processed â†’ cleaned, analytics-ready datasets

ğŸ—„ï¸ Data Warehouse â€“ PostgreSQL

Example table schema:

CREATE TABLE stock_prices (
  symbol TEXT,
  price NUMERIC,
  currency TEXT,
  timestamp TIMESTAMP,
  ingest_date DATE
);


The table is populated automatically via Airflowâ€™s S3ToSqlOperator.

ğŸ³ Dockerized Setup
Services

Airflow Webserver

Airflow Scheduler

MinIO

PostgreSQL

Start the project
docker compose up -d --build

Access UIs
Service	URL
Airflow	http://localhost:8080

MinIO	http://localhost:9001

Postgres	localhost:5432


### ğŸ“Œ What This Project Demonstrates

âœ” Real-world Airflow DAG design
âœ” API-based data ingestion
âœ” S3-compatible object storage usage
âœ” Dockerized data engineering workflows
âœ” Warehouse loading patterns
âœ” Production-style ETL orchestration

### ğŸš€ Future Enhancements

Add data quality checks (Great Expectations)

Partitioned warehouse tables

Backfill support

Alerting via Slack/Email

Schema evolution handling

CI/CD for DAG deployment

## ğŸ‘¨â€ğŸ’» Author

Prathik Bharath Jain

Data Engineer | Data Scientist | Software Engineer

MS in Data Science â€“ University of Colorado Boulder